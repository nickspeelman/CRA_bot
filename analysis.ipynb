{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CRA_bot Analysis\n",
    "## Housekeeping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sql_conn import sql_conn\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up some data frames we'll use later"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# set up one dataframe to rule them all\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# set up a bot df\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "where\n",
    "    s.subject_type = 'bot'\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "bot_df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# set up a human df\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "where\n",
    "    s.subject_type = 'human'\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "human_df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's get some summary statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    571.000000\n",
      "mean       3.075306\n",
      "std        0.762914\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "total_scores = df['score_weight']\n",
    "\n",
    "print(total_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The distribution of all scores is heavily skewed to the left"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    183.000000\n",
      "mean       2.890710\n",
      "std        0.851031\n",
      "min        1.000000\n",
      "25%        2.000000\n",
      "50%        3.000000\n",
      "75%        3.500000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "human_scores = human_df['score_weight']\n",
    "\n",
    "print(human_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interestinglly the mean is smaller here, but the distribution is heavily skewed to the left"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    388.000000\n",
      "mean       3.162371\n",
      "std        0.702102\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "bot_scores = bot_df['score_weight']\n",
    "\n",
    "print(bot_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The bots have outscored the humans\n",
    "\n",
    "#### But is it significant?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  -4.023318525326921  | P-Value:  6.513635076731906e-05\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_scores, bot_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### It looks like it is significant.\n",
    "\n",
    "### Does the type of question matter?\n",
    "\n",
    "#### Let's look at analogies first"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    41.000000\n",
      "mean      2.634146\n",
      "std       0.766684\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_analogy_scores = human_df.loc[human_df['question_type'] == 'Analogy', 'score_weight']\n",
    "\n",
    "print(human_analogy_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    62.000000\n",
      "mean      2.951613\n",
      "std       0.894930\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       4.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_analogy_scores = human_df.loc[bot_df['question_type'] == 'Analogy', 'score_weight']\n",
    "\n",
    "print(bot_analogy_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  -1.8631913103793505  | P-Value:  0.06534093195300862\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_analogy_scores, bot_analogy_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bots outscored humans on analogies, but it is not significant.\n",
    "\n",
    "#### What of ambiguous prompts?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    79.000000\n",
      "mean      3.000000\n",
      "std       0.905822\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       4.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_ambiguous_scores = human_df.loc[human_df['question_type'] == 'Ambiguous', 'score_weight']\n",
    "\n",
    "print(human_ambiguous_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    125.000000\n",
      "mean       2.992000\n",
      "std        0.827901\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_ambiguous_scores = bot_df.loc[bot_df['question_type'] == 'Ambiguous', 'score_weight']\n",
    "\n",
    "print(bot_ambiguous_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  0.06480934991287637  | P-Value:  0.9483898888827076\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_ambiguous_scores, bot_ambiguous_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Humans outscored bots on Ambiguity but it's not significant\n",
    "\n",
    "#### What about Novel question types?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    63.000000\n",
      "mean      2.920635\n",
      "std       0.809253\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_novel_scores = human_df.loc[human_df['question_type'] == 'Novel', 'score_weight']\n",
    "\n",
    "print(human_novel_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    59.000000\n",
      "mean      2.813559\n",
      "std       0.819474\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_novel_scores = human_df.loc[bot_df['question_type'] == 'Novel', 'score_weight']\n",
    "\n",
    "print(bot_novel_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  0.7258904034748159  | P-Value:  0.4693193378629199\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_novel_scores, bot_novel_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Humans outscored bots on novel words, but it is not significant.\n",
    "\n",
    "### In summary, we can say that bots outscored humans as whole with confidence, but as far as drilling down into the specific question types, we can't say much.\n",
    "\n",
    "## Let's make some visualizations at the highest level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'human_summary_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# histogram of human scores\u001B[39;00m\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[1;32m----> 4\u001B[0m plt\u001B[38;5;241m.\u001B[39mhist(\u001B[43mhuman_summary_scores\u001B[49m, bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, edgecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblack\u001B[39m\u001B[38;5;124m'\u001B[39m, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHistogram of Human Summary Scores\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHuman Summary Scores\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'human_summary_scores' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of human scores\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(human_summary_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Human Summary Scores')\n",
    "plt.xlabel('Human Summary Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histogram of bot scores\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bot_summary_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Bot Summary Scores')\n",
    "plt.xlabel('Bot Summary Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's put those on the same plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the histograms\n",
    "human_hist, human_bins = np.histogram(human_summary_scores, bins=20)\n",
    "bot_hist, bot_bins = np.histogram(bot_summary_scores, bins=20)\n",
    "\n",
    "# scale them\n",
    "human_hist_percentage = human_hist / human_hist.sum() * 100\n",
    "bot_hist_percentage = bot_hist / bot_hist.sum() * 100\n",
    "\n",
    "# define the width of the bars\n",
    "width = (human_bins[1] - human_bins[0]) / 1.5\n",
    "\n",
    "# plot the histograms side by side\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot human summary scores\n",
    "plt.bar(human_bins[:-1] - width / 2, human_hist_percentage, width=width, color='#5B3341', alpha=0.7, edgecolor='black', label='Human Summary Scores')\n",
    "\n",
    "# plot bot summary scores\n",
    "plt.bar(bot_bins[:-1] + width / 2, bot_hist_percentage, width=width, color='#BB315F', alpha=0.7, edgecolor='black', label='Bot Summary Scores')\n",
    "\n",
    "plt.title('Histogram of Summary Scores as Percentages')\n",
    "plt.xlabel('Summary Scores')\n",
    "plt.ylabel('Percentage of Total')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "# set x-axis ticks to only whole numbers\n",
    "plt.xticks(np.arange(min(human_bins), max(human_bins) + 1, 1))\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### That's pretty useful. Let's gussy it up a bit to put it in my presentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the histograms as percentages\n",
    "bins = np.arange(min(human_summary_scores.min(), bot_summary_scores.min()), max(human_summary_scores.max(), bot_summary_scores.max()) + 2) - 0.5\n",
    "human_hist, human_bins = np.histogram(human_summary_scores, bins=bins)\n",
    "bot_hist, bot_bins = np.histogram(bot_summary_scores, bins=bins)\n",
    "\n",
    "# scale them\n",
    "human_hist_percentage = human_hist / human_hist.sum() * 100\n",
    "bot_hist_percentage = bot_hist / bot_hist.sum() * 100\n",
    "\n",
    "# define the column widths\n",
    "width = (human_bins[1] - human_bins[0]) * 0.35\n",
    "\n",
    "# adjust x-coordinates to center the labels\n",
    "x_ticks = human_bins[:-1] + 0.5\n",
    "\n",
    "# create traces\n",
    "trace1 = go.Bar(\n",
    "    x=human_bins[:-1] - width / 2 + 0.5,\n",
    "    y=human_hist_percentage,\n",
    "    width=width,\n",
    "    name='Human Summary Scores',\n",
    "    marker=dict(color='#5B3341')\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=bot_bins[:-1] + width / 2 + 0.5,\n",
    "    y=bot_hist_percentage,\n",
    "    width=width,\n",
    "    name='Bot Summary Scores',\n",
    "    marker=dict(color='#BB315F')\n",
    ")\n",
    "\n",
    "# create the layout\n",
    "layout = go.Layout(\n",
    "    title='Histogram of Summary Scores as Percentages',\n",
    "    xaxis=dict(\n",
    "        title='Summary Scores',\n",
    "        tickmode='array',\n",
    "        tickvals=x_ticks,\n",
    "        ticktext=[str(int(x)) for x in x_ticks],\n",
    "    ),\n",
    "    yaxis=dict(title='Percentage of Total'),\n",
    "    barmode='overlay',\n",
    "    bargap=0.2,\n",
    "    legend=dict(title='Type', x=0.85, y=1.15),\n",
    "    template='plotly_white',\n",
    "    font=dict(family='Sagona Book', size=14)\n",
    ")\n",
    "\n",
    "# create the figure\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "# save the figure as an image\n",
    "fig.write_image(\"histogram.png\")\n",
    "\n",
    "# show the figure\n",
    "fig.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
