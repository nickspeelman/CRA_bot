{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CRA_bot Analysis\n",
    "## Housekeeping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sql_conn import sql_conn\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import scipy.stats as stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up some data frames we'll use later"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# set up one dataframe to rule them all\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# set up a bot df\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "where\n",
    "    s.subject_type = 'bot'\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "bot_df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# set up a human df\n",
    "\n",
    "sql = '''\n",
    "    select\n",
    "    s.subject_id,\n",
    "    s.subject,\n",
    "    s.subject_type,\n",
    "    q.question_id,\n",
    "    q.question,\n",
    "    qt.question_type_id,\n",
    "    qt.question_type,\n",
    "    r.response_id,\n",
    "    r.response,\n",
    "    sc.score_id,\n",
    "    sc.score_weight\n",
    "from\n",
    "    subjects s\n",
    "    inner join\n",
    "        responses r on s.subject_id = r.subject_id\n",
    "    inner join\n",
    "        questions q on r.question_id = q.question_id\n",
    "    inner join\n",
    "        question_types qt on q.question_type_id = qt.question_type_id\n",
    "    inner join\n",
    "        response_scores rs on r.response_id = rs.response_id\n",
    "    inner join\n",
    "        scores sc on rs.score_id = sc.score_id\n",
    "where\n",
    "    s.subject_type = 'human'\n",
    "'''\n",
    "\n",
    "conn, cursor = sql_conn('responses.db')\n",
    "\n",
    "human_df = pd.read_sql(sql,conn)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's get some summary statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    571.000000\n",
      "mean       3.075306\n",
      "std        0.762914\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "total_scores = df['score_weight']\n",
    "\n",
    "print(total_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The distribution of all scores is heavily skewed to the left"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    183.000000\n",
      "mean       2.890710\n",
      "std        0.851031\n",
      "min        1.000000\n",
      "25%        2.000000\n",
      "50%        3.000000\n",
      "75%        3.500000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "human_scores = human_df['score_weight']\n",
    "\n",
    "print(human_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interestinglly the mean is smaller here, but the distribution is heavily skewed to the left"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    388.000000\n",
      "mean       3.162371\n",
      "std        0.702102\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# total score summary\n",
    "\n",
    "bot_scores = bot_df['score_weight']\n",
    "\n",
    "print(bot_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The bots have outscored the humans\n",
    "\n",
    "#### But is it significant?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  -4.023318525326921  | P-Value:  6.513635076731906e-05\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_scores, bot_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### It looks like it is significant.\n",
    "\n",
    "### Does the type of question matter?\n",
    "\n",
    "#### Let's look at analogies first"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    41.000000\n",
      "mean      2.634146\n",
      "std       0.766684\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_analogy_scores = human_df.loc[human_df['question_type'] == 'Analogy', 'score_weight']\n",
    "\n",
    "print(human_analogy_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    62.000000\n",
      "mean      2.951613\n",
      "std       0.894930\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       4.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_analogy_scores = human_df.loc[bot_df['question_type'] == 'Analogy', 'score_weight']\n",
    "\n",
    "print(bot_analogy_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  -1.8631913103793505  | P-Value:  0.06534093195300862\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_analogy_scores, bot_analogy_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bots outscored humans on analogies, but it is not significant.\n",
    "\n",
    "#### What of ambiguous prompts?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    79.000000\n",
      "mean      3.000000\n",
      "std       0.905822\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       4.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_ambiguous_scores = human_df.loc[human_df['question_type'] == 'Ambiguous', 'score_weight']\n",
    "\n",
    "print(human_ambiguous_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    125.000000\n",
      "mean       2.992000\n",
      "std        0.827901\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_ambiguous_scores = bot_df.loc[bot_df['question_type'] == 'Ambiguous', 'score_weight']\n",
    "\n",
    "print(bot_ambiguous_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  0.06480934991287637  | P-Value:  0.9483898888827076\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_ambiguous_scores, bot_ambiguous_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Humans outscored bots on Ambiguity but it's not significant\n",
    "\n",
    "#### What about Novel question types?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    63.000000\n",
      "mean      2.920635\n",
      "std       0.809253\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "human_novel_scores = human_df.loc[human_df['question_type'] == 'Novel', 'score_weight']\n",
    "\n",
    "print(human_novel_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    59.000000\n",
      "mean      2.813559\n",
      "std       0.819474\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       4.000000\n",
      "Name: score_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bot_novel_scores = human_df.loc[bot_df['question_type'] == 'Novel', 'score_weight']\n",
    "\n",
    "print(bot_novel_scores.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Stat:  0.7258904034748159  | P-Value:  0.4693193378629199\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_value = ttest_ind(human_novel_scores, bot_novel_scores)\n",
    "\n",
    "\n",
    "print('T-Stat: ', t_stat, \" | P-Value: \", p_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Humans outscored bots on novel words, but it is not significant.\n",
    "\n",
    "### In summary, we can say that bots outscored humans as whole with confidence, but as far as drilling down into the specific question types, we can't say much.\n",
    "\n",
    "## Let's make some visualizations at the highest level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'human_summary_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# histogram of human scores\u001B[39;00m\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[1;32m----> 4\u001B[0m plt\u001B[38;5;241m.\u001B[39mhist(\u001B[43mhuman_summary_scores\u001B[49m, bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, edgecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblack\u001B[39m\u001B[38;5;124m'\u001B[39m, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHistogram of Human Summary Scores\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHuman Summary Scores\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'human_summary_scores' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of human scores\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(human_summary_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Human Summary Scores')\n",
    "plt.xlabel('Human Summary Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histogram of bot scores\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bot_summary_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Bot Summary Scores')\n",
    "plt.xlabel('Bot Summary Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's put those on the same plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the histograms\n",
    "human_hist, human_bins = np.histogram(human_summary_scores, bins=20)\n",
    "bot_hist, bot_bins = np.histogram(bot_summary_scores, bins=20)\n",
    "\n",
    "# scale them\n",
    "human_hist_percentage = human_hist / human_hist.sum() * 100\n",
    "bot_hist_percentage = bot_hist / bot_hist.sum() * 100\n",
    "\n",
    "# define the width of the bars\n",
    "width = (human_bins[1] - human_bins[0]) / 1.5\n",
    "\n",
    "# plot the histograms side by side\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot human summary scores\n",
    "plt.bar(human_bins[:-1] - width / 2, human_hist_percentage, width=width, color='#5B3341', alpha=0.7, edgecolor='black', label='Human Summary Scores')\n",
    "\n",
    "# plot bot summary scores\n",
    "plt.bar(bot_bins[:-1] + width / 2, bot_hist_percentage, width=width, color='#BB315F', alpha=0.7, edgecolor='black', label='Bot Summary Scores')\n",
    "\n",
    "plt.title('Histogram of Summary Scores as Percentages')\n",
    "plt.xlabel('Summary Scores')\n",
    "plt.ylabel('Percentage of Total')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "# set x-axis ticks to only whole numbers\n",
    "plt.xticks(np.arange(min(human_bins), max(human_bins) + 1, 1))\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### That's pretty useful. Let's gussy it up a bit to put it in my presentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the histograms as percentages\n",
    "bins = np.arange(min(human_summary_scores.min(), bot_summary_scores.min()), max(human_summary_scores.max(), bot_summary_scores.max()) + 2) - 0.5\n",
    "human_hist, human_bins = np.histogram(human_summary_scores, bins=bins)\n",
    "bot_hist, bot_bins = np.histogram(bot_summary_scores, bins=bins)\n",
    "\n",
    "# scale them\n",
    "human_hist_percentage = human_hist / human_hist.sum() * 100\n",
    "bot_hist_percentage = bot_hist / bot_hist.sum() * 100\n",
    "\n",
    "# define the column widths\n",
    "width = (human_bins[1] - human_bins[0]) * 0.35\n",
    "\n",
    "# adjust x-coordinates to center the labels\n",
    "x_ticks = human_bins[:-1] + 0.5\n",
    "\n",
    "# create traces\n",
    "trace1 = go.Bar(\n",
    "    x=human_bins[:-1] - width / 2 + 0.5,\n",
    "    y=human_hist_percentage,\n",
    "    width=width,\n",
    "    name='Human Summary Scores',\n",
    "    marker=dict(color='#5B3341')\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=bot_bins[:-1] + width / 2 + 0.5,\n",
    "    y=bot_hist_percentage,\n",
    "    width=width,\n",
    "    name='Bot Summary Scores',\n",
    "    marker=dict(color='#BB315F')\n",
    ")\n",
    "\n",
    "# create the layout\n",
    "layout = go.Layout(\n",
    "    title='Histogram of Summary Scores as Percentages',\n",
    "    xaxis=dict(\n",
    "        title='Summary Scores',\n",
    "        tickmode='array',\n",
    "        tickvals=x_ticks,\n",
    "        ticktext=[str(int(x)) for x in x_ticks],\n",
    "    ),\n",
    "    yaxis=dict(title='Percentage of Total'),\n",
    "    barmode='overlay',\n",
    "    bargap=0.2,\n",
    "    legend=dict(title='Type', x=0.85, y=1.15),\n",
    "    template='plotly_white',\n",
    "    font=dict(family='Sagona Book', size=14)\n",
    ")\n",
    "\n",
    "# create the figure\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "# save the figure as an image\n",
    "fig.write_image(\"histogram.png\")\n",
    "\n",
    "# show the figure\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can we see anything about individuals?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subject_id subject_type  mean_score_weight\n",
      "0          756          bot           3.571429\n",
      "1          754          bot           3.382353\n",
      "2          752          bot           3.290323\n",
      "3          755          bot           3.235294\n",
      "4          759        human           3.225806\n",
      "5          747          bot           3.161290\n",
      "6          751          bot           3.151515\n",
      "7          760        human           3.117647\n",
      "8          753          bot           3.093750\n",
      "9          746          bot           3.093750\n",
      "10         745          bot           3.058824\n",
      "11         749          bot           3.030303\n",
      "12         757        human           3.000000\n",
      "13         761        human           3.000000\n",
      "14         762        human           2.939394\n",
      "15         750          bot           2.935484\n",
      "16         748          bot           2.857143\n",
      "17         763        human           2.576923\n",
      "18         758        human           2.555556\n"
     ]
    }
   ],
   "source": [
    "mean_scores = df.groupby(['subject_id','subject_type'])['score_weight'].mean().reset_index()\n",
    "\n",
    "mean_scores.rename(columns={'score_weight': 'mean_score_weight'}, inplace=True)\n",
    "\n",
    "mean_scores_sorted = mean_scores.sort_values(by='mean_score_weight', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(mean_scores_sorted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-value: 5.2499946643426885\n",
      "P-value: 0.034990018771146776\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.pivot_table(values='score_weight', index='subject_id', columns='subject_type')\n",
    "\n",
    "# Perform ANOVA\n",
    "f_value, p_value = stats.f_oneway(*[pivot_df[col].dropna() for col in pivot_df.columns])\n",
    "\n",
    "print(f\"F-value: {f_value}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The four best performing models are all bots, and the ANOVA has a p of 0.3. This confirms what we already knew.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   temp  mean_score_weight\n",
      "0   1.0           3.201550\n",
      "1   0.5           3.161538\n",
      "2   0.0           3.124031\n"
     ]
    }
   ],
   "source": [
    "bot_df['temp'] = bot_df['subject'].str.extract(r'\\|\\s*([\\d\\.]+)$')\n",
    "\n",
    "bot_df['temp'] = bot_df['temp'].astype(float)\n",
    "\n",
    "mean_temp_scores = bot_df.groupby(['temp'])['score_weight'].mean().reset_index()\n",
    "\n",
    "mean_temp_scores.rename(columns={'score_weight': 'mean_score_weight'}, inplace=True)\n",
    "\n",
    "mean_temp_scores_sorted = mean_temp_scores.sort_values(by='mean_score_weight', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(mean_temp_scores_sorted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-value: 0.39204414986658276\n",
      "P-value: 0.6759437111837998\n"
     ]
    }
   ],
   "source": [
    "grouped_data = bot_df.groupby('temp')['score_weight'].apply(list)\n",
    "\n",
    "valid_groups = [group for group in grouped_data if len(group) > 1]\n",
    "\n",
    "if len(valid_groups) > 1:\n",
    "    f_value, p_value = stats.f_oneway(*valid_groups)\n",
    "    print(f\"F-value: {f_value}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "else:\n",
    "    print(\"Not enough groups with multiple values to perform ANOVA.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### There's no meaningful difference in the results of the temperature value on understanding"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
